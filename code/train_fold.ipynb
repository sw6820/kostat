{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEjQ58PIBNIK"
      },
      "source": [
        "### Import & Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIiyGCVVR5_6",
        "outputId": "04aa9b88-217e-4ab6-8be0-b544405a84a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Apr  7 13:58:43 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# T4보다 P100이 더 빠릅니다.\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBT_NKZfpC5k",
        "outputId": "035e3ea0-c653-4150-c1fc-c6253d37e755"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsKnIIZgpDum",
        "outputId": "f153d2d6-9b79-4f2b-c35c-49e3085912d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/industry_classification\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/industry_classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yb4C_a3_gAYp",
        "outputId": "68e4405a-a125-4d66-bc32-7c35c21c1911"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.0 MB 4.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 77 kB 7.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 60.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 85.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 64.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 325 kB 4.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 72.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 136 kB 80.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 86.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 56.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 79.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 126 kB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 75.2 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 4.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 68.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 74.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.2 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers -qq\n",
        "!pip install datasets -qq\n",
        "!pip install wandb -qq\n",
        "!pip install scikit-learn -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "mLoL4NC8pMEp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "import torch\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "from transformers import TrainingArguments, Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "sw1Y4OTQtD5i"
      },
      "outputs": [],
      "source": [
        "from logger import get_logger\n",
        "from preprocess import Preprocess\n",
        "from model import Model\n",
        "from loss import FocalLoss\n",
        "from dataset import IndustryDataset\n",
        "from label_encoder import get_label_encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBnKoGOBLalv",
        "outputId": "6742a5b8-bbc8-4967-e46f-f7a45b89461f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "def seed_everything(seed) :\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed) # if use multi-GPU\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "seed_everything(42)\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "TYpJHmY3L__h"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "#############\n",
        "fold 번호를 입력해주세요! (0~4)\n",
        "#############\n",
        "'''\n",
        "fold_num = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Vga0ldzeiu4q"
      },
      "outputs": [],
      "source": [
        "# root logger setting\n",
        "import logging\n",
        "FORMAT = '%(asctime)s - %(name)s | %(levelname)s - %(message)s'\n",
        "logging.basicConfig(filename=f\"roberta-base_{fold_num}.log\", format=FORMAT, level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojl95jv-BV4S"
      },
      "source": [
        "### Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "tubUf1YEq4lE"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('data/1. 실습용자료.txt', sep='|', encoding='cp949')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZosDHBunvdoR",
        "outputId": "592630b7-f3c5-44d0-8e33-e7203db52ee8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-07 14:08:35,777 - preprocess | INFO - Success train data Preprocessing\n",
            "2022-04-07 14:08:35,777 - preprocess | INFO - Success train data Preprocessing\n"
          ]
        }
      ],
      "source": [
        "preprocesser = Preprocess()\n",
        "train = preprocesser.train_preprocess(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "FyAWvMyM5BVo"
      },
      "outputs": [],
      "source": [
        "train_dataset, eval_dataset = train_test_split(train, test_size=0.2, stratify=train[\"digit_1\"], random_state=42)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "eval_dataset = eval_dataset.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0-8tK53_vLb"
      },
      "source": [
        "### Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMqi0HmxGZkQ",
        "outputId": "bfabc584-9685-49d9-c89a-460ef9a98aee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([224, 122, 119, ..., 124, 208, 145])"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label_encoder = get_label_encoder()\n",
        "train_encoded = label_encoder.transform(train[\"label\"])\n",
        "train_encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f30B2NVK_zGN"
      },
      "source": [
        "### Load Pretrained Model, Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rskkeEUNelQp",
        "outputId": "38a2db03-8f9c-4e00-b836-e26781985d73"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/klue/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a96469ca2a238496d435a0e9e202f261119c146a0326444b6d68ae1adc35e04f.85b0b02ba2a483f3adb8a60ab70dbd875768fcd5e6cdb21a593c6e02fdffac3a\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"klue/roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BertTokenizer\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/klue/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/b204e0dc0a3b8fd45b35e7fcefd97c5f839b86c14aea510f1eb38fb8469e23d8.57d3cd0dfa80e5a249a776870dc87b6da993900685a271086750174009115320\n",
            "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "2022-04-07 14:08:41,396 - model | INFO - Success Loading Model: klue/roberta-base\n",
            "2022-04-07 14:08:41,396 - model | INFO - Success Loading Model: klue/roberta-base\n",
            "loading file https://huggingface.co/klue/roberta-base/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/e8441a174492958462b6b16b6db8f1e7253cd149ca779522cadd812d55091b89.d1b86bed49516351c7bb29b19d7e7be2ab53b931bcb1f9b2aacfb71f2124d25a\n",
            "loading file https://huggingface.co/klue/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/233a5b2c17873a8477b62dd92a02092a9937759e924a5f22b111becebb8aba5e.44c30ade4958fcfd446e66025e10a5b380cdd0bbe9b3fb7a794f357e7f0f34c2\n",
            "loading file https://huggingface.co/klue/roberta-base/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/klue/roberta-base/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/9d0c87e44b00acfbfbae931b2e4068eb6311a0c3e71e23e5400bdf57cab4bfbf.70c17d6e4d492c8f24f5bb97ab56c7f272e947112c6faf9dd846da42ba13eb23\n",
            "loading file https://huggingface.co/klue/roberta-base/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/9a9f77abeddd1bbd8de28608e78dd3604287ad91abd4796cd25ad936715b7640.5b0ba083b234382bb4c99ee0c9f4fca4cadaa053dd17c32dabfe0de2f629af1f\n",
            "2022-04-07 14:08:46,698 - model | INFO - Success Loading tokenizer: klue/roberta-base\n",
            "2022-04-07 14:08:46,698 - model | INFO - Success Loading tokenizer: klue/roberta-base\n"
          ]
        }
      ],
      "source": [
        "model_name = \"klue/roberta-base\"\n",
        "model_info = Model(model_name)\n",
        "model = model_info.get_model()\n",
        "tokenizer = model_info.get_tokenizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDdu0UNKABm6"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "VDrLiFbhkbSH"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from torch.utils.data.dataset import Subset\n",
        "\n",
        "kfd = StratifiedKFold(n_splits=5, shuffle=False)\n",
        "train[\"label\"] = label_encoder.transform(train[\"label\"])\n",
        "train_data = IndustryDataset(train, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "X2gu7ww_og1d"
      },
      "outputs": [],
      "source": [
        "# fold 생성, digit_1을 기준으로 stratified fold\n",
        "train_idx = []\n",
        "val_idx = []\n",
        "for (train, val) in kfd.split(train[\"text\"], train[\"digit_1\"]):\n",
        "  train_idx.append(train)\n",
        "  val_idx.append(val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "C-_d1T5wo3A3"
      },
      "outputs": [],
      "source": [
        "train_set = Subset(train_data, train_idx[fold_num])\n",
        "val_set = Subset(train_data, val_idx[fold_num])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "kD_fzeV2KZyX"
      },
      "outputs": [],
      "source": [
        "# https://huggingface.co/course/chapter3/3?fw=pt\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
        "# 2개 이상 metric 정상 적용 X\n",
        "\n",
        "from datasets import load_metric\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "# acuracy_metric = load_metric('accuracy')\n",
        "# f1_metric = load_metric('f1')\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits = eval_pred.predictions\n",
        "    labels = eval_pred.label_ids\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    label_indices=list(range(len(labels)))\n",
        "    # # accuracy = acuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "    # # f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"micro\", labels=label_indices)[\"f1\"]\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average=\"macro\", labels=label_indices)\n",
        "    return {\"accuracy\": accuracy, \"f1\": f1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "RcO0X0XKI3IW"
      },
      "outputs": [],
      "source": [
        "class CustomTrainer(Trainer):\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "\n",
        "  def compute_loss(self, model, inputs, return_outputs=False):\n",
        "    custom_loss = FocalLoss()\n",
        "    labels = inputs.pop(\"labels\")\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    if labels is not None:\n",
        "      loss = custom_loss(outputs.get('logits'), labels)\n",
        "      loss = loss.mean()\n",
        "    else:\n",
        "      loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
        "    \n",
        "    return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "A52jiQfPKhYf"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3IBwyJWK2Iv",
        "outputId": "a8d9ada2-04bc-4709-8870-a028df25ca3a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
            "PyTorch: setting up devices\n"
          ]
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()\n",
        "\n",
        "training_args=TrainingArguments(output_dir=f\"./roberta-base_{fold_num}\",\n",
        "                                num_train_epochs=5,\n",
        "                                learning_rate=5e-5,\n",
        "                                save_total_limit=3,\n",
        "                                # save_steps=3000,\n",
        "                                per_device_train_batch_size=128,\n",
        "                                per_device_eval_batch_size=128,\n",
        "                                evaluation_strategy='epoch',\n",
        "                                save_strategy='epoch',\n",
        "                                # eval_steps = 3000,\n",
        "                                logging_first_step=True,\n",
        "                                logging_dir=\"./\",\n",
        "                                logging_steps=100,\n",
        "                                seed=42,\n",
        "                                weight_decay=0.01,\n",
        "                                load_best_model_at_end = True,\n",
        "                                report_to=\"wandb\",\n",
        "                                run_name=f\"./roberta-base_{fold_num}\")\n",
        "trainer = CustomTrainer(model=model,\n",
        "                  args=training_args,\n",
        "                  train_dataset=train_set,\n",
        "                  eval_dataset=val_set,\n",
        "                  tokenizer=tokenizer,\n",
        "                  compute_metrics=compute_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "qE3u0soxM8Mg"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "K-mj5LhzM6SW",
        "outputId": "db2eec66-1070-4760-b693-3be7c8e67b67"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 800000\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 128\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 31250\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='65' max='31250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   65/31250 01:36 < 13:18:19, 0.65 it/s, Epoch 0.01/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_result = trainer.train() \n",
        "metrics = train_result.metrics\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "\n",
        "model.save_pretrained(f\"./roberta-base_{fold_num}/result/best_model\")\n",
        "logging.info(\"Success model trained\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "train_fold",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
