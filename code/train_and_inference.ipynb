{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEjQ58PIBNIK"
      },
      "source": [
        "### Import & Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIiyGCVVR5_6",
        "outputId": "4106f917-940d-4519-b6f3-25743ca38662"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Apr  2 10:58:12 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# T4보다 P100이 더 빠릅니다.\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBT_NKZfpC5k",
        "outputId": "08f85cfb-d668-4124-a2b6-0f100034c8c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsKnIIZgpDum",
        "outputId": "f91365c9-b8c8-4159-a4fd-b32dcb6494bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/industry_classification\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/industry_classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yb4C_a3_gAYp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b3cbc76-da18-43b9-ff3d-391b0070a1d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.8 MB 7.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 67.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 64.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 55.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 325 kB 7.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 67.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 136 kB 76.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 69.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 86.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 4.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 68.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 78.9 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 6.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 92.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 90.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers -qq\n",
        "!pip install datasets -qq\n",
        "!pip install wandb -qq\n",
        "!pip install scikit-learn -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mLoL4NC8pMEp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "import torch\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "from transformers import TrainingArguments, Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sw1Y4OTQtD5i"
      },
      "outputs": [],
      "source": [
        "from logger import get_logger\n",
        "from preprocess import Preprocess\n",
        "from model import Model\n",
        "from loss import FocalLoss\n",
        "from dataset import IndustryDataset\n",
        "from label_encoder import get_label_encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBnKoGOBLalv",
        "outputId": "42fc222a-cef2-4cb8-e070-b7c0ed2a8648"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "def seed_everything(seed) :\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed) # if use multi-GPU\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "seed_everything(42)\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Vga0ldzeiu4q"
      },
      "outputs": [],
      "source": [
        "# root logger setting\n",
        "import logging\n",
        "FORMAT = '%(asctime)s - %(name)s | %(levelname)s - %(message)s'\n",
        "logging.basicConfig(filename=\"run.log\", format=FORMAT, level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojl95jv-BV4S"
      },
      "source": [
        "### Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tubUf1YEq4lE"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('data/1. 실습용자료.txt', sep='|', encoding='cp949')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZosDHBunvdoR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "728f079e-ea82-4d10-d50d-0ef9762acf2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-02 10:59:28,326 - preprocess | INFO - Success train data Preprocessing\n"
          ]
        }
      ],
      "source": [
        "preprocesser = Preprocess()\n",
        "train = preprocesser.train_preprocess(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FyAWvMyM5BVo"
      },
      "outputs": [],
      "source": [
        "train_dataset, eval_dataset = train_test_split(train, test_size=0.2, stratify=train[\"digit_1\"], random_state=42)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "eval_dataset = eval_dataset.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0-8tK53_vLb"
      },
      "source": [
        "### Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMqi0HmxGZkQ",
        "outputId": "ab120d2f-04e5-49c2-d5ef-e7f956d2e8b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([224, 122, 119, ..., 124, 208, 145])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "label_encoder = get_label_encoder()\n",
        "train_encoded = label_encoder.transform(train[\"label\"])\n",
        "train_encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f30B2NVK_zGN"
      },
      "source": [
        "### Load Pretrained Model, Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rskkeEUNelQp",
        "outputId": "e5db735d-8982-40ed-b7c2-16c2dc0e2604"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file https://huggingface.co/klue/roberta-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6aa0817a5b48e36ec821c3d9cb8267f2dda41a32b317311688f06c742b6b2a1d.7f009d6b27d54554dce063a5fc8273742e9a198f6f627aada4bc5ea2dbbc0313\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"klue/roberta-small\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BertTokenizer\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/klue/roberta-small/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/fafa42c8bb6f4b1313506774e000bc35d156dce6eb02c75a5e4a37405e0bac4a.57e9f25d17aa6a6c07d65c9c33f5319ede6e71bccfe0f3910dddf6a7598919d9\n",
            "Some weights of the model checkpoint at klue/roberta-small were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "2022-04-02 12:22:59,532 - model | INFO - Success Loading Model: klue/roberta-small\n",
            "2022-04-02 12:22:59,532 - model | INFO - Success Loading Model: klue/roberta-small\n",
            "loading file https://huggingface.co/klue/roberta-small/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/bdc674caf9fa028a31519ca20a1382eb8a04fd24d25e53cf739fdce04040a860.d1b86bed49516351c7bb29b19d7e7be2ab53b931bcb1f9b2aacfb71f2124d25a\n",
            "loading file https://huggingface.co/klue/roberta-small/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/53e1834914ad16bfb05201950994b448fbe4afaac39ec44ce2d02a1c7c27e20d.44c30ade4958fcfd446e66025e10a5b380cdd0bbe9b3fb7a794f357e7f0f34c2\n",
            "loading file https://huggingface.co/klue/roberta-small/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/klue/roberta-small/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/9ce71a5afff600bb47488785ec31125c4a485302e21d660291b10925f8bfcb67.70c17d6e4d492c8f24f5bb97ab56c7f272e947112c6faf9dd846da42ba13eb23\n",
            "loading file https://huggingface.co/klue/roberta-small/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ba870c5a767afeacf58a256013d35f5293df17fa80a89b7b82e28ee08e290a27.5b0ba083b234382bb4c99ee0c9f4fca4cadaa053dd17c32dabfe0de2f629af1f\n",
            "2022-04-02 12:23:01,458 - model | INFO - Success Loading tokenizer: klue/roberta-small\n",
            "2022-04-02 12:23:01,458 - model | INFO - Success Loading tokenizer: klue/roberta-small\n"
          ]
        }
      ],
      "source": [
        "# \"monologg/kobert\", \"monologg/kodistilbert\" evaluation 후 vocab 저장 과정에서 오류 확인\n",
        "model_name = \"klue/roberta-small\"\n",
        "model_info = Model(model_name)\n",
        "model = model_info.get_model()\n",
        "tokenizer = model_info.get_tokenizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kbrbmz_o_9Jb"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8tC2Hr5WISxK"
      },
      "outputs": [],
      "source": [
        " train_dataset[\"label\"] = label_encoder.transform(train_dataset[\"label\"])\n",
        " eval_dataset[\"label\"] = label_encoder.transform(eval_dataset[\"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Cg9FwIRSJcPX"
      },
      "outputs": [],
      "source": [
        "train_dataset = IndustryDataset(train_dataset, tokenizer)\n",
        "eval_dataset = IndustryDataset(eval_dataset, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDdu0UNKABm6"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "kD_fzeV2KZyX"
      },
      "outputs": [],
      "source": [
        "# https://huggingface.co/course/chapter3/3?fw=pt\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
        "# 2개 이상 metric 정상 적용 X\n",
        "\n",
        "from datasets import load_metric\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "# acuracy_metric = load_metric('accuracy')\n",
        "# f1_metric = load_metric('f1')\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits = eval_pred.predictions\n",
        "    labels = eval_pred.label_ids\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    label_indices=list(range(len(labels)))\n",
        "    # # accuracy = acuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "    # # f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"micro\", labels=label_indices)[\"f1\"]\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average=\"micro\", labels=label_indices)\n",
        "    return {\"accuracy\": accuracy, \"f1\": f1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "RcO0X0XKI3IW"
      },
      "outputs": [],
      "source": [
        "class CustomTrainer(Trainer):\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "\n",
        "  def compute_loss(self, model, inputs, return_outputs=False):\n",
        "    custom_loss = FocalLoss()   \n",
        "    labels = inputs.pop(\"labels\")\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    if labels is not None:\n",
        "      loss = custom_loss(outputs.get('logits'), labels)\n",
        "      loss = loss.mean()\n",
        "    else:\n",
        "      loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
        "    \n",
        "    return (loss, outputs) if return_outputs else loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "A52jiQfPKhYf"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3IBwyJWK2Iv",
        "outputId": "c61c67f3-29eb-46ae-cf11-c9713de887df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
            "PyTorch: setting up devices\n"
          ]
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()\n",
        "\n",
        "training_args=TrainingArguments(output_dir=\"./test\",\n",
        "                                num_train_epochs=1,\n",
        "                                learning_rate=5e-5,\n",
        "                                save_total_limit=3,\n",
        "                                # save_steps=3000,\n",
        "                                per_device_train_batch_size=128,\n",
        "                                per_device_eval_batch_size=128,\n",
        "                                evaluation_strategy='epoch',\n",
        "                                save_strategy='epoch',\n",
        "                                # eval_steps = 3000,\n",
        "                                logging_first_step=True,\n",
        "                                logging_dir=\"./\",\n",
        "                                logging_steps=100,\n",
        "                                seed=42,\n",
        "                                weight_decay=0.01,\n",
        "                                load_best_model_at_end = True,\n",
        "                                report_to=\"wandb\",\n",
        "                                run_name=\"test\")\n",
        "trainer = CustomTrainer(model=model,\n",
        "                  args=training_args,\n",
        "                  train_dataset=train_dataset,\n",
        "                  eval_dataset=eval_dataset,\n",
        "                  tokenizer=tokenizer,\n",
        "                  compute_metrics=compute_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "qE3u0soxM8Mg"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        },
        "id": "K-mj5LhzM6SW",
        "outputId": "68874b3c-592d-4d7b-cf00-c06de88ab68b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 800000\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 128\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 6250\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/industry_classification/wandb/run-20220402_110104-7wpl4ocg</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/jdg4661/huggingface/runs/7wpl4ocg\" target=\"_blank\">test</a></strong> to <a href=\"https://wandb.ai/jdg4661/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6250/6250 54:03, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.227800</td>\n",
              "      <td>0.211098</td>\n",
              "      <td>0.914950</td>\n",
              "      <td>0.914950</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200000\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to ./test/checkpoint-6250\n",
            "Configuration saved in ./test/checkpoint-6250/config.json\n",
            "Model weights saved in ./test/checkpoint-6250/pytorch_model.bin\n",
            "tokenizer config file saved in ./test/checkpoint-6250/tokenizer_config.json\n",
            "Special tokens file saved in ./test/checkpoint-6250/special_tokens_map.json\n",
            "Deleting older checkpoint [test/checkpoint-3] due to args.save_total_limit\n",
            "Deleting older checkpoint [test/checkpoint-6] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./test/checkpoint-6250 (score: 0.21109837293624878).\n",
            "Configuration saved in ./roberta-small/result/best_model/config.json\n",
            "Model weights saved in ./roberta-small/result/best_model/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "train_result = trainer.train() \n",
        "metrics = train_result.metrics\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "\n",
        "model.save_pretrained('./roberta-small/result/best_model')\n",
        "logging.info(\"Success model trained\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "qpU_1nc4MqBP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oqck93PTiu4w"
      },
      "outputs": [],
      "source": [
        "submission_name = \"roberta-small_test\"\n",
        "inference_model = './roberta-small/checkpoint-30000'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FliKjwUliu4x"
      },
      "outputs": [],
      "source": [
        "from inference import Inference\n",
        "\n",
        "test = pd.read_csv('data/2. 모델개발용자료.txt', sep='|', encoding='cp949')\n",
        "submission = pd.read_csv(\"data/답안 작성용 파일.csv\", encoding='cp949')\n",
        "\n",
        "# Inference(test, inference_model의 위치, submission df파일)\n",
        "inference_model = Inference(test, inference_model, submission)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "aCFq2Coaiu4x",
        "outputId": "78c9248c-d832-408e-b00a-b2aa828cb6e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-02 01:33:36,414 - model | INFO - Success Loading Model: ./roberta-small/checkpoint-30000\n",
            "2022-04-02 01:33:36,414 - model | INFO - Success Loading Model: ./roberta-small/checkpoint-30000\n",
            "2022-04-02 01:33:37,840 - model | INFO - Success Loading tokenizer: ./roberta-small/checkpoint-30000\n",
            "2022-04-02 01:33:37,840 - model | INFO - Success Loading tokenizer: ./roberta-small/checkpoint-30000\n",
            "2022-04-02 01:33:50,649 - preprocess | INFO - Success test data Preprocessing\n",
            "***** Running Prediction *****\n",
            "  Num examples = 100000\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12500' max='12500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [12500/12500 03:25]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-02 01:39:53,017 - inference | INFO - Success decoding\n",
            "2022-04-02 01:39:53,019 - inference | INFO - Success inference: ./roberta-small/checkpoint-30000\n"
          ]
        }
      ],
      "source": [
        "# 혹시 UnboundedLocalError 발생시 한 번 더 실행해주세요\n",
        "result = inference_model.inference()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWQ9d0ZwCw3w"
      },
      "outputs": [],
      "source": [
        "result.to_csv(f\"{submission_name}.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "train_and_inference.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}